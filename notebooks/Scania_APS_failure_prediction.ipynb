{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                Sensor Component Failure Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Problem statement.\n",
    "\n",
    "**Data:** Sensor Data\n",
    "\n",
    "**Problem statement :**\n",
    "- The system in focus is the Air Pressure system (APS) which generates pressurized air that are utilized in various functions in a truck, such as braking and gear changes. The datasets positive class corresponds to component failures for a specific component of the APS system. The negative class corresponds to trucks with failures for components not related to the APS system.\n",
    "\n",
    "- The problem is to reduce the cost due to unnecessary repairs. So it is required to minimize the false predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|True class | Positive | Negative | |\n",
    "| ----------- | ----------- |   |  |\n",
    "|<b>Predicted class</b>||| |\n",
    "| Positive      |   -       | cost_1  |    |\n",
    "| Negative   | cost_2        |  | |\n",
    "\n",
    "\n",
    "Cost 1 = 10 and Cost 2 = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The total cost of a prediction model the sum of `Cost_1` multiplied by the number of Instances with type 1 failure and `Cost_2` with the number of instances with type 2 failure, resulting in a `Total_cost`. In this case `Cost_1` refers to the cost that an unnessecary check needs to be done by an mechanic at an workshop, while `Cost_2` refer to the cost of missing a faulty truck, which may cause a breakdown. \n",
    "- `Total_cost = Cost_1 * No_Instances + Cost_2 * No_Instances.`\n",
    "\n",
    "- From the above problem statement we could observe that, we have to reduce false positives and false negatives. More importantly we have to **reduce false negatives, since cost incurred due to false negative is 50 times higher than the false positives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges and other objectives\n",
    "\n",
    "- Need to Handle many Null values in almost all columns\n",
    "- No low-latency requirement.\n",
    "- Interpretability is not important.\n",
    "- misclassification leads the unecessary repair costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \\\n",
    "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve,confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import  train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "df = pd.read_csv('aps_failure_training_reduced.csv', na_values=\"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rows and columns of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of target varaible\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical & categorical columns\n",
    "numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "\n",
    "# print columns\n",
    "print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As this is a Sensor data. Interpretation of the data is not required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Missing values count for each column\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "missing = df.isna().sum().div(df.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)\n",
    "\n",
    "ax.bar(missing.index, missing.values.T[0])\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Percentage missing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns which has more than 70% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Dropping columns which has more than 70% of missing values\n",
    "dropcols = missing[missing[0]>70]\n",
    "dropcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(list(dropcols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of the dataset after dropping columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the total percentage of missing values of full dataset after dropping columns with more than 70% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count= df.isnull().sum()\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "print(f\"Percentage of total missing cells in the data {(total_missing/total_cells) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of unique values in Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df['class']=='pos'].shape[0]\n",
    "neg = df[df['class']=='neg'].shape[0]\n",
    "print(\"Positive: \" + str(pos) + \", Negative: \" + str(neg))\n",
    "sns.catplot(data=df, x=\"class\", kind=\"count\", palette=\"winter_r\", alpha=.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "- The target classes are highly imbalanced\n",
    "- Class imbalance is a scenario that arises when we have unequal distribution of class in a dataset i.e. the no. of data points in the negative class (majority class) very large compared to that of the positive class (minority class)\n",
    "- If the imbalanced data is not treated beforehand, then this will degrade the performance of the classifier model. \n",
    "- Hence we should handle imbalanced data with certain methods.\n",
    "\n",
    "**How to handle Imbalance Data ?**\n",
    "\n",
    "- Resampling data is one of the most commonly preferred approaches to deal with an imbalanced dataset. There are broadly two types of methods for this i) Undersampling ii) Oversampling. In most cases, oversampling is preferred over undersampling techniques. The reason being, in undersampling we tend to remove instances from data that may be carrying some important information.\n",
    "- **SMOTE:** Synthetic Minority Oversampling Technique\n",
    "- SMOTE is an oversampling technique where the synthetic samples are generated for the minority class.\n",
    "- Hybridization techniques involve combining both undersampling and oversampling techniques. This is done to optimize the performance of classifier models for the samples created as part of these techniques.\n",
    "- It only duplicates the data and it won't add and new information. Hence we look at some different techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(true, predicted):\n",
    "    '''\n",
    "    This function takes in true values and predicted values\n",
    "    Returns: Accuracy, F1-Score, Precision, Recall, Roc-auc Score\n",
    "    '''\n",
    "    acc = accuracy_score(true, predicted) # Calculate Accuracy\n",
    "    f1 = f1_score(true, predicted) # Calculate F1-score\n",
    "    precision = precision_score(true, predicted) # Calculate Precision\n",
    "    recall = recall_score(true, predicted)  # Calculate Recall\n",
    "    roc_auc = roc_auc_score(true, predicted) #Calculate Roc\n",
    "    return acc, f1 , precision, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost of the model as per data description\n",
    "def total_cost(y_true, y_pred):\n",
    "    '''\n",
    "    This function takes y_ture, y_predicted, and prints Total cost due to misclassification\n",
    "   \n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    cost = 10*fp + 500*fn\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which can evaluate models and return a report \n",
    "def evaluate_models(X, y, models):\n",
    "    '''\n",
    "    This function takes in X and y and models dictionary as input\n",
    "    It splits the data into Train Test split\n",
    "    Iterates through the given model dictionary and evaluates the metrics\n",
    "    Returns: Dataframe which contains report of all models metrics with cost\n",
    "    '''\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    cost_list=[]\n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Training set performance\n",
    "        model_train_accuracy, model_train_f1,model_train_precision,\\\n",
    "        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)\n",
    "        train_cost = total_cost(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "        # Test set performance\n",
    "        model_test_accuracy,model_test_f1,model_test_precision,\\\n",
    "        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)\n",
    "        test_cost = total_cost(y_test, y_test_pred)\n",
    "\n",
    "        print(list(models.keys())[i])\n",
    "        models_list.append(list(models.keys())[i])\n",
    "\n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_train_f1)) \n",
    "        print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "        print(f'- COST: {train_cost}.')\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "        print('Model performance for Test set')\n",
    "        print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "        print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "        print(f'- COST: {test_cost}.')\n",
    "        cost_list.append(test_cost)\n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "        \n",
    "    report=pd.DataFrame(list(zip(models_list, cost_list)), columns=['Model Name', 'Cost']).sort_values(by=[\"Cost\"])\n",
    "        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot  distribution of all Independent Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "\n",
    "plt.figure(figsize=(15, 100))\n",
    "for i, col in enumerate(numeric_features):\n",
    "    plt.subplot(60, 3, i+1)\n",
    "    sns.distplot(x=df[col], color='indianred')\n",
    "    plt.xlabel(col, weight='bold')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "- As per the above plot most of the features are not normally distributed.\n",
    "- Transformation of data is not of prime importance since it is a classification problem.\n",
    "- Interpreting each and every column is not necessary as this is sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on Different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and y for all Experiments\n",
    "X= df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Manually Encoding Target Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y= y.replace({'pos': 1, 'neg': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: 1 = KNN Imputer for Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Robust scaler and not Standard scaler?**\n",
    "- Scaling the data using Robust scaler\n",
    "- Since most of the independent variables are not normally distributed we cannot use Standardscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Robust Scaler and not Minmax?** \n",
    "- because most of the feature has outliers. So Minmax will scale data according to Max values which is outlier.\n",
    "- This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with robust scaler for KNN best K-selection experminet\n",
    "robustscaler = RobustScaler()\n",
    "X1 = robustscaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why KNN Imputer**?\n",
    "- KNNImputer by scikit-learn is a widely used method to impute missing values. It is widely being observed as a replacement for traditional imputation techniques.\n",
    "- KNNImputer helps to impute missing values present in the observations by finding the nearest neighbors with the Euclidean distance matrix.\n",
    "- Here we Iterates through different K values and get accuracy and choose best K values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the optimal n_neighbour value for KNN imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "# define imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "strategies = [str(i) for i in [1,3,5,7,9]]\n",
    "for s in strategies:\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', LogisticRegression())])\n",
    "    scores = cross_val_score(pipeline, X1, y, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "    results.append(scores)\n",
    "    print('n_neighbors= %s || accuracy (%.4f)' % (s , mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can observe n_neighbors=3 able to produce highest accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Fit the KNN imputer with selected K-value\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3)),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn =knn_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **SMOTE+TOMEK** is one of such a hybrid technique that aims to clean overlapping data points for each of the classes distributed in sample space.\n",
    " \n",
    " - This method combines the SMOTE ability to generate synthetic data for minority class and Tomek Links ability to remove the data that are identified as Tomek links from the majority class\n",
    " \n",
    " - To add new data of minority class\n",
    " 1. Choose random data from the minority class.\n",
    " 2. Calculate the distance between the random data and its k nearest neighbors.\n",
    " 3. Multiply the difference with a random number between 0 and 1, then add the result to the minority class as a synthetic sample.\n",
    " 4. Repeat step number 2–3 until the desired proportion of minority class is met.\n",
    " \n",
    " - To remove the tomek links of the majority class\n",
    " 1. Choose random data from the majority class.\n",
    " 2. If the random data’s nearest neighbor is the data from the minority class (i.e. create the Tomek Link), then remove the Tomek Link.\n",
    " \n",
    " - This is method instead of adding duplicate data it synthesises the new data based on the already avalialble classes. Hence we choose this as our imputer method for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority',n_jobs=-1)\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_knn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Default Models in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary which contains models for experiment\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "     \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"XGBClassifier\": XGBClassifier(), \n",
    "     \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit KNN imputed data for models in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_knn = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report for KNN Imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- For the Experiment 1: Knn imputer has XGBoost classifier as the best Model\n",
    "- Proceeding with further experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: 2 = Simple Imputer with Strategy Median "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SimpleImputer is a class in the `sklearn.impute` module that can be used to replace missing values in a dataset, using a variety of input strategies.\n",
    "- Here we use SimpleImputer can also be used to impute multiple columns at once by passing in a list of column names. SimpleImputer will then replace missing values in all of the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Fit the Simple imputer with strategy median\n",
    "median_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X with median_pipeline\n",
    "X_median = median_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority')\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_median, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the models\n",
    "report_median = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for Simple Imputer with median strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- For the Experiment 2: Simple imputer with median strategy has Catboost classifier as the best Model\n",
    "- Proceeding with further experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: 3 = MICE for Imputing Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MICE stands for Multivariate Imputation By Chained Equations algorithm\n",
    "- This technique by which we can effortlessly impute missing values in a dataset by looking at data from other columns and trying to estimate the best prediction for each missing value.\n",
    "- `ImputationKernel` Creates a kernel dataset. This dataset can perform MICE on itself, and impute new data from models obtained during MICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf\n",
    "\n",
    "X_mice = X.copy()\n",
    "kernel = mf.ImputationKernel(\n",
    "  X_mice,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1989\n",
    ")# Run the MICE algorithm for 3 iterations kernel.mice(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mice = kernel.complete_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit robust scaler\n",
    "mice_pipeline = Pipeline(steps=[\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X with Mice imputer \n",
    "X_mice= mice_pipeline.fit_transform(X_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority', n_jobs=-1 )\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_mice, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the models\n",
    "report_mice = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for MICE Imputer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- For the Experiment 3: Mice imputer has XGBoost classifier as the best Model\n",
    "- Proceeding with further experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: 4 = Simple Imputer with Strategy Constant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another strategy which can be used is replacing missing values with a fixed (constant) value.\n",
    "- To do this, specify “constant” for strategy and specify the fill value using the fill_value parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with simple imputer with strategy constant and fill value 0\n",
    "constant_pipeline = Pipeline(steps=[\n",
    "    ('Imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_const =constant_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority', n_jobs=-1 )\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_const, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training the models\n",
    "report_const = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for Simple Imputer with Constant strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "- For the Experiment 4: Simple imputer with constant strategy has XGBoost classifier as the best Model\n",
    "- Proceeding with further experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: 5 = Simple Imputer with Strategy Mean \n",
    "\n",
    "- Another strategy which can be used is replacing missing values with mean\n",
    "- Here we replace the missing values with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with Simple imputer with strategy mean\n",
    "mean_pipeline = Pipeline(steps=[\n",
    "    ('Imputer', SimpleImputer(strategy='mean')),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = mean_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority' , n_jobs=-1)\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_mean, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training all models\n",
    "report_mean = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report for Simple imputer with strategy mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: 5 = Principle component analysis with imputing median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! \n",
    "- As the dataset has 164 columns we can try PCA and check our metrics Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipeline = Pipeline(steps=[\n",
    "    ('Imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "var_ratio={}\n",
    "for n in range(2,150):\n",
    "    pc=PCA(n_components=n)\n",
    "    df_pca=pc.fit(X_pca)\n",
    "    var_ratio[n]=sum(df_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting variance ratio\n",
    "pd.Series(var_ratio).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kneed algorithm to find the elbow point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "i = np.arange(len(var_ratio))\n",
    "variance_ratio= list(var_ratio.values())\n",
    "components=  list(var_ratio.keys())\n",
    "knee = KneeLocator(i, variance_ratio, S=1, curve='concave', interp_method='polynomial')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "k= components[knee.knee]\n",
    "print('Knee Locator k =', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dimensions of the data \n",
    "pca_final=PCA(n_components=18,random_state=42).fit(X_res)\n",
    "\n",
    "reduced=pca_final.fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority', n_jobs=-1)\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training all models\n",
    "report_pca = evaluate_models(X_res,y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report for PCA and Mean imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt=PrettyTable()\n",
    "pt.field_names=[\"Model\",\"Imputation_method\",\"Total_cost\"]\n",
    "pt.add_row([\"XGBClassifier\",\"Simple Imputer-Constant\",\"2950\"])\n",
    "pt.add_row([\"XGBClassifier\",\"Mice\",\"3510\"])\n",
    "pt.add_row([\"XGBClassifier\",\"Knn-Imputer\",\"4460\"])\n",
    "pt.add_row([\"XGBClassifier\",\"Simple Imputer-Mean\",\"4950\"])\n",
    "pt.add_row([\"CatBoostClassifier\",\"Median\",\"5760\"])\n",
    "pt.add_row([\"Random Forest\",\"PCA\",\"34150\"])\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "- From the final report we can see than XGBClassifier with Simple imputer with strategy constant has performed the best with cost of 2950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Final model and get reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = XGBClassifier()\n",
    "\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority', n_jobs=-1)\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_const, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.2,random_state=42)\n",
    "\n",
    "final_model = final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final XGBoost Classifier Accuracy Score (Train) :\", final_model.score(X_train,y_train))\n",
    "print(\"Final XGBoost Classifier Accuracy Score (Test) :\", accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final XGBoost Classifier Cost Metric(Test) :\",total_cost(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#plots Confusion matrix\n",
    "plot_confusion_matrix(final_model, X_test, y_test, cmap='Blues', values_format='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best Model is XGBoost Classifier with 99.6% accuracy and cost of 2950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
